{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8d78a4cd",
   "metadata": {},
   "source": [
    "# Lab - Parallel Computing Part 2 - GPU Kernel Timing\n",
    "\n",
    "## E6692 Spring 2022\n",
    "\n",
    "Complete **Part 1: Define GPU Kernel Functions for a Deep Learning Model** before starting this part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21cfd5f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "\n",
    "from utils.context import Context, GPUKernels\n",
    "from utils.plot_execution_times import plot_execution_times, INPUT_SIZES\n",
    "\n",
    "# define GPU\n",
    "device = torch.device('cuda')\n",
    "\n",
    "# define block size\n",
    "BLOCK_SIZE = 32\n",
    "\n",
    "# define timing iterations\n",
    "iterations = 10\n",
    "\n",
    "# define kernel path\n",
    "kernel_path = './kernels.cu'\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c09ea5ae",
   "metadata": {},
   "source": [
    "## Part 2: GPU Kernel Timing\n",
    "\n",
    "Now that we have implemented CUDA versions of deep learning network layers and verified the results against PyTorch's versions, we will compare the execution times of our CUDA implementation against PyTorch CPU and PyTorch GPU execution times. PyTorch simplifies almost all of the GPU memory allocation and context management. Data is sent to the GPU with the **.to(device)** method, where device is the **torch.device()** object defined in the first cell that corresponds to the Jetson Nano's GPU. The following cell gives an example of how to time operations and execute PyTorch operations on CPU vs GPU with the **relu()** function. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "536b4de2",
   "metadata": {},
   "source": [
    "### ReLU Timing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0c2fc0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "context = Context(BLOCK_SIZE)\n",
    "source_module = context.getSourceModule(kernel_path)\n",
    "cuda_functions = GPUKernels(context, source_module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11ba9f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# relu time profiling example\n",
    "\n",
    "# define timing lists\n",
    "time_pytorch_cpu = []\n",
    "time_pytorch_gpu = []\n",
    "time_cuda = []\n",
    "\n",
    "for input_size in INPUT_SIZES:\n",
    "    \n",
    "    time_pytorch_cpu_total = 0\n",
    "    time_pytorch_gpu_total = 0\n",
    "    time_cuda_total = 0\n",
    "    \n",
    "    # average execution times for more stable and accurate results\n",
    "    for _ in range(iterations):\n",
    "    \n",
    "        # define input array\n",
    "        input_array = np.random.randint(-10, high=10, size=(input_size, input_size))\n",
    "\n",
    "        # define PyTorch input array\n",
    "        input_array_pytorch_cpu = torch.from_numpy(input_array)\n",
    "\n",
    "        # profile CPU PyTorch\n",
    "        pytorch_cpu_start = time.time()\n",
    "        output_array_pytorch_cpu = F.relu(input_array_pytorch_cpu)\n",
    "        pytorch_cpu_end = time.time()\n",
    "\n",
    "        # add pytorch CPU time to list\n",
    "        time_pytorch_cpu_total += (pytorch_cpu_end - pytorch_cpu_start) * 1000\n",
    "\n",
    "        # profile GPU PyTorch (including memory transfer)\n",
    "        pytorch_gpu_start = time.time()\n",
    "        input_array_pytorch_gpu = input_array_pytorch_cpu.to(device)\n",
    "        output_array_pytorch_gpu = F.relu(input_array_pytorch_gpu)\n",
    "        output_array_pytorch_gpu = output_array_pytorch_gpu.cpu()\n",
    "        pytorch_gpu_end = time.time()\n",
    "\n",
    "        # add pytorch GPU time to list\n",
    "        time_pytorch_gpu_total += (pytorch_gpu_end - pytorch_gpu_start) * 1000\n",
    "\n",
    "        # profile CUDA (including memory transfer)\n",
    "        cuda_start = time.time()\n",
    "        cuda_output = cuda_functions.relu(input_array)\n",
    "        cuda_end = time.time()\n",
    "\n",
    "        # add CUDA time to list\n",
    "        time_cuda_total += (cuda_end - cuda_start) * 1000\n",
    "        \n",
    "    time_pytorch_cpu.append(time_pytorch_cpu_total / iterations)\n",
    "    time_pytorch_gpu.append(time_pytorch_gpu_total / iterations)\n",
    "    time_cuda.append(time_cuda_total / iterations)\n",
    "\n",
    "# plot times\n",
    "title = \"ReLU Execution Time by Method\"\n",
    "plot_execution_times(time_pytorch_cpu, time_pytorch_gpu, time_cuda, title)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ca8bde1",
   "metadata": {},
   "source": [
    "### Discuss the results of the relu() time profile:\n",
    "\n",
    "#### 1. Are the GPU implementations faster than PyTorch CPU implementation? Why or why not?\n",
    "\n",
    "TODO: Your answer here\n",
    "\n",
    "#### 2. How does our CUDA implementation or ReLU compare to PyTorch's GPU implementation?\n",
    "\n",
    "TODO: Your answer here\n",
    "\n",
    "#### 3. Based on 1, is it worth computing ReLU activation by itself on GPU? If not, why is this function still needed for GPU deep learning implementations?\n",
    "\n",
    "TODO: Your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f40a5aa2",
   "metadata": {},
   "source": [
    "### Timing of 2D Convolution, 2D Max-Pooling, and Fully Connected layers\n",
    "\n",
    "TODO: Using the relu time profile as an example, generate time profile plots using **plot_execution_times()** for **conv2d()**, **MaxPool2d()**, and **linear()**. Each CUDA implementation function should be compared to the corresponding CPU and GPU implementations in PyTorch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74c5340a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: conv2d() time profile and plot. You may define any mask shape.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d3bec30",
   "metadata": {},
   "source": [
    "### Discuss the results of the conv2d() time profile.\n",
    "\n",
    "TODO: Your discussion here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4593b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: MaxPool2d() time profile and plot. You may use any pooling kernel size.\n",
    "\n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0961a57",
   "metadata": {},
   "source": [
    "### Discuss the results of the MaxPool2d() time profile.\n",
    "\n",
    "TODO: Your discussion here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08fa03f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: linear() time profile and plot. Define weight and bias shapes corresponding \n",
    "#       to the input size\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e21f3da",
   "metadata": {},
   "source": [
    "### Discuss the results of the linear() time profile.\n",
    "\n",
    "TODO: Your discussion here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
