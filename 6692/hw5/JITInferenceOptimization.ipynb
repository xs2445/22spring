{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "79083b84",
   "metadata": {},
   "source": [
    "## PyTorch Inference Optimization \n",
    "\n",
    "## E6692 Spring 2022"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c212eb21",
   "metadata": {},
   "source": [
    "In this notebook you will measure the throughput of your custom trained YOLOv4-Tiny model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "409d2415",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import torch # import modules\n",
    "import sys\n",
    "import time\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "from darknet_utils.darknet_to_pytorch import load_pytorch, load_darknet_as_pytorch # import custom modules\n",
    "from darknet_utils.inference import measure_throughput, plot_execution_times\n",
    "\n",
    "cfg_path = './cfg/yolov4-tiny-person-vehicle.cfg' # path to configuration file of custom model\n",
    "\n",
    "torch_weights_path = './darknet/backup/yolov4-tiny-person-vehicle_final.weights' # path to JIT optimized model\n",
    "\n",
    "device = 'cuda'\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae7b62fc",
   "metadata": {},
   "source": [
    "**TODO:** Load the custom trained YOLOv4-Tiny PyTorch model in the cell below. \n",
    "\n",
    "TIP: If you don't want to see the model summary, you can insert `%%capture` as the first line in the cell to hide all output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d7890940",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%capture\n",
    "# TODO: load the custom trained YOLOv4-Tiny with load_pytorch()\n",
    "\n",
    "# pytorch_model = load_pytorch(cfg_path, torch_weights_path)\n",
    "pytorch_model = load_darknet_as_pytorch(cfg_path, torch_weights_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22ccca72",
   "metadata": {},
   "source": [
    "### Define throughput in the context of deep learning models.\n",
    "\n",
    "**TODO:** Your answer here.\n",
    "\n",
    "Throughput refers to the number of data units processed in one unit of time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89e05c15",
   "metadata": {},
   "source": [
    "**TODO:** Complete the function **measure_throughput()** in **darknet_utils/inference.py**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5fc9ebc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "115.84283724449033"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: use measure_throughput() to measure the throughput of your PyTorch YOLOv4-Tiny\n",
    "#       what is a reasonable input shape to measure throughput?\n",
    "measure_throughput(pytorch_model, (1,3,480,640))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0fdf627",
   "metadata": {},
   "source": [
    "Now you will investigate the PyTorch [JIT](https://en.wikipedia.org/wiki/Just-in-time_compilation) functionality to further optimize YOLOv4-Tiny for inference on the Jetson Nano. [PyTorch JIT](https://pytorch.org/docs/stable/jit.html) has two methods for converting a standard PyTorch model to a [TorchScript](https://pytorch.org/docs/stable/jit.html#:~:text=TorchScript%20is%20a%20way%20to,there%20is%20no%20Python%20dependency.) model - script and trace. Torchscript models are optimizable and serializable. That means they can be saved independently from Python and used in other contexts like a C++ program. \n",
    "\n",
    "### What is meant by eager execution and graph execution?\n",
    "\n",
    "**TODO:** Your answer here.\n",
    "\n",
    "### Explain the difference between \"script\" and \"trace\". When would you use jit.script and when would you use jit.trace? Why do you need to include a sample input when using jit.trace?\n",
    "\n",
    "**TODO:** Your answer here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d816e5ed",
   "metadata": {},
   "source": [
    "**TODO:** Use `torch.jit.trace()` to create a traced TorchScript model. Save the traced model weights to the weights directory with `torch.jit.save()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3c1bb3d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Use torch.jit to generate a traced PyTorch version of YOLOv4-Tiny.\n",
    "\n",
    "save_path = './weights/jit_trace_model_weights.pt'\n",
    "\n",
    "example_forward_input = torch.rand(1,3,480,640).to(device)\n",
    "pytorch_model.cuda()\n",
    "# traced_model = torch.jit.trace()\n",
    "traced_model = torch.jit.trace(pytorch_model.forward, example_forward_input)\n",
    "\n",
    "# traced_model\n",
    "torch.jit.save(traced_model, save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e4d4de03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JIT traced model matches PyTorch:  True\n"
     ]
    }
   ],
   "source": [
    "# Verify that output of traced model matches output of original model\n",
    "\n",
    "input_array = torch.randn((4, 3, 480, 640)).to(device) # define input tensor\n",
    "\n",
    "torch_output = pytorch_model(input_array)[0].cpu().detach().numpy() # pass input through models\n",
    "jit_output = traced_model(input_array)[0].cpu().detach().numpy()\n",
    "\n",
    "print(\"JIT traced model matches PyTorch: \", np.allclose(torch_output, jit_output)) # compare outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7184b328",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "132.20748047196855"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: use measure_throughput() to measure the throughput of your traced YOLOv4-Tiny\n",
    "measure_throughput(traced_model, (1,3,480,640))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8d907d4",
   "metadata": {},
   "source": [
    "**TODO:** Calculate and plot throughput as a function of batch size for both models. Calculate throughput for the largest batch size you can fit into the Jetson Nano's memory. You're welcome to use **plot_execution_times()** or your own plotting function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4651750",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: calculate throughput as a function of batch size for both models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83937c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: plot throughput as a function of batch size for both models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e645674",
   "metadata": {},
   "source": [
    "### Discuss the results of your throughput measurements. Are there any surprising results? Which batch sizes result in the largest throughputs? Is this batch size reasonable to use in real-time applications?\n",
    "\n",
    "**TODO:** Your answer here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df2253b9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
