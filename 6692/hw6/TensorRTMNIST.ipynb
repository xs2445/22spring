{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "222321e7",
   "metadata": {},
   "source": [
    "## Lab - TensorRT + Profiling - TensorRT Python API\n",
    "## E6692 Spring 2022"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c31a262",
   "metadata": {},
   "source": [
    "Using this notebook and the python script **pytorch_inference.py** you will load the trained weights generated in **TrainPytorchMNIST.ipynb** into a TensorRT model. TensorRT is a framework and C++ library developed by NVIDIA for model deployment. It offers high performance inference optimization for NVIDIA GPUs (like the Jetson Nano, T4 on GCP, etc.), which is why we're interested in it. \n",
    "\n",
    "To get a general feel for TensorRT, I recommend starting [here](https://developer.nvidia.com/tensorrt). Then it would be helpful to read the [How TensorRT Works](https://docs.nvidia.com/deeplearning/tensorrt/developer-guide/index.html#work) section of the TRT docs, and finally the [TensorRT's Capabilities](https://docs.nvidia.com/deeplearning/tensorrt/developer-guide/index.html#fit) and [The TensorRT Python API](https://docs.nvidia.com/deeplearning/tensorrt/developer-guide/index.html#python_topics) will guide you as you work on this lab.\n",
    "\n",
    "In this first part you will be defining a model structure using the TensorRT Python API. Then you will load the trained PyTorch weights into your TensorRT model to perform inference optimizations. The architecture of the TensorRT model needs to be identical to the PyTorch model defined in **pyTorchCNN.py**, otherwise the weights will not transfer successfully. You can print the model summary of the PyTorch MnistClassifier to use as a blueprint when defining the TensorRT model with the Python API. \n",
    "\n",
    "You will need to review the [documentation of the Python TensorRT API](https://docs.nvidia.com/deeplearning/tensorrt/api/python_api/index.html). \n",
    "**TODO:** Use the Python API to define the MNIST classifier structure with the function **generate_serialized_trt_engine()** in **tensorRTCNN.py**. Use the function comments as a guide. The cell below can be used to print the model summary of the PyTorch MnistClassifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f3362d32",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorrt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_15218/2496093034.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtensorrt\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtrt\u001b[0m \u001b[0;31m# import modules\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_serialized_engine\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mserializeMNIST\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mserializeMNIST\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrt_prediction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorrt'"
     ]
    }
   ],
   "source": [
    "import tensorrt as trt # import modules\n",
    "\n",
    "from utils.utils import load_serialized_engine\n",
    "from serializeMNIST import serializeMNIST, trt_prediction\n",
    "\n",
    "trained_pytorch_weights = './weights/trained_pytorch_weights' # define pytorch weights path, change if necessary\n",
    "trt_mnist_engine = './engines/trt_model.engine' # define the tensorrt serialized engine path\n",
    "\n",
    "TEST_CASES = 100 # test case iterations\n",
    "\n",
    "TRT_LOGGER = trt.Logger(trt.Logger.WARNING) # defnie trt logger object with warnings enabled\n",
    "runtime = trt.Runtime(TRT_LOGGER) # define runtime context\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da225e2c",
   "metadata": {},
   "source": [
    "**TODO:** Use the Python API to define the MNIST classifier structure with the function **generate_serialized_trt_engine()** in **tensorRTCNN.py**. Use the function comments as a guide. The cell below can be used to print the model summary of the PyTorch MnistClassifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "881e13e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: it may be helpful to print the MnistClassifier summary here as a reference to the model structure\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "182a3f04",
   "metadata": {},
   "source": [
    "**TODO:** After you have finished implementing the TRT MNIST classifier in **generate_serialized_trt_engine()**, use **serializeMNIST()** to (i) load the trained PyTorch weights, (ii) generate the serialized engine, and (iii) save the serialized engine to a file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ac7b1c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: call serializeMNIST() to generate the serialized MNIST classifier TRT engine\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33f280c2",
   "metadata": {},
   "source": [
    "As you've learned, the Jetson Nano has very limited memory resources. It's therefore a good idea to restart the kernel with `Kernel` --> `Restart Kernel...` to free as much memory as possible for the next steps. You might not need to, but if you run into OOM errors, try restarting here. Once you have generated the serial engine file you do not need to repeat the weight loading and inference optimization process. \n",
    "\n",
    "**TODO:** Load the serialized engine in the cell below with **load_serialized_engine()**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52206354",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Use load_serialized_engine() to load the serialized engine into memory\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57bd14df",
   "metadata": {},
   "source": [
    "**TODO:** Deserialize the TRT engine with [**runtime.deserialize_cuda_engine()**](https://docs.nvidia.com/deeplearning/tensorrt/api/python_api/infer/Core/Runtime.html#tensorrt.Runtime.deserialize_cuda_engine)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3760c6ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: generate an ICudaEngine with runtime.deserialize_cuda_engine()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d3408d3",
   "metadata": {},
   "source": [
    "**TODO:** Calculate the accuracy of the TensorRT MNIST Classifier on `TEST_CASE` validation images. You can use **trt_prediction()** in **serializeMNIST.py** to generate random ground truth and predictions, or you can use **allocate_buffers()** and **do_inference()** to allocate input/output memory buffers and make predictions without choosing validation images randomly. Either approach is fine here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43a7d895",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: calculate accuracy of TRT MNIST classifier on the validation set\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1f1e8e6",
   "metadata": {},
   "source": [
    "## Discussion\n",
    "\n",
    "### In what situation would converting a model to TensorRT be useful?\n",
    "\n",
    "**TODO:** Your answer here.\n",
    "\n",
    "### How does the PyTorch model validation accuracy compare to the TensorRT model validation accuracy?\n",
    "\n",
    "**TODO:** Your answer here.\n",
    "\n",
    "### Briefly explain how to following inference optimization techniques can increase the throughput of a model.\n",
    "\n",
    "#### Reduced Floating Point Precision\n",
    "\n",
    "**TODO:** Your answer here.\n",
    "\n",
    "#### Layer Fusion\n",
    "\n",
    "**TODO:** Your answer here.\n",
    "\n",
    "#### Dynamic Tensor Memory\n",
    "\n",
    "**TODO:** Your answer here.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "332ae154",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
