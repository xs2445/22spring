{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "15a863ac",
   "metadata": {},
   "source": [
    "## Lab - TensorRT + Profiling - TensorRT Python API\n",
    "## E6692 Spring 2022"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f1ff30c",
   "metadata": {},
   "source": [
    "Using this notebook and the python script **pytorch_inference.py** you will load the trained weights generated in **TrainPytorchMNIST.ipynb** into a TensorRT model. TensorRT is a framework and C++ library developed by NVIDIA for model deployment. It offers high performance inference optimization for NVIDIA GPUs (like the Jetson Nano, T4 on GCP, etc.), which is why we're interested in it. \n",
    "\n",
    "To get a general feel for TensorRT, I recommend starting [here](https://developer.nvidia.com/tensorrt). Then it would be helpful to read the [How TensorRT Works](https://docs.nvidia.com/deeplearning/tensorrt/developer-guide/index.html#work) section of the TRT docs, and finally the [TensorRT's Capabilities](https://docs.nvidia.com/deeplearning/tensorrt/developer-guide/index.html#fit) and [The TensorRT Python API](https://docs.nvidia.com/deeplearning/tensorrt/developer-guide/index.html#python_topics) will guide you as you work on this lab.\n",
    "\n",
    "In this first part you will be defining a model structure using the TensorRT Python API. Then you will load the trained PyTorch weights into your TensorRT model to perform inference optimizations. The architecture of the TensorRT model needs to be identical to the PyTorch model defined in **pyTorchCNN.py**, otherwise the weights will not transfer successfully. You can print the model summary of the PyTorch MnistClassifier to use as a blueprint when defining the TensorRT model with the Python API. \n",
    "\n",
    "You will need to review the [documentation of the Python TensorRT API](https://docs.nvidia.com/deeplearning/tensorrt/api/python_api/index.html). \n",
    "**TODO:** Use the Python API to define the MNIST classifier structure with the function **generate_serialized_trt_engine()** in **tensorRTCNN.py**. Use the function comments as a guide. The cell below can be used to print the model summary of the PyTorch MnistClassifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5ca807b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorrt as trt # import modules\n",
    "\n",
    "from utils.utils import load_serialized_engine\n",
    "from serializeMNIST import serializeMNIST, trt_prediction\n",
    "\n",
    "trained_pytorch_weights = './weights/mnist.pt' # define pytorch weights path, change if necessary\n",
    "trt_mnist_engine = './engines/trt_model.engine' # define the tensorrt serialized engine path\n",
    "\n",
    "TEST_CASES = 100 # test case iterations\n",
    "\n",
    "TRT_LOGGER = trt.Logger(trt.Logger.WARNING) # defnie trt logger object with warnings enabled\n",
    "runtime = trt.Runtime(TRT_LOGGER) # define runtime context\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e85d9570",
   "metadata": {},
   "source": [
    "**TODO:** Use the Python API to define the MNIST classifier structure with the function **generate_serialized_trt_engine()** in **tensorRTCNN.py**. Use the function comments as a guide. The cell below can be used to print the model summary of the PyTorch MnistClassifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "99b1fc15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias'])\n"
     ]
    }
   ],
   "source": [
    "# TODO: it may be helpful to print the MnistClassifier summary here as a reference to the model structure\n",
    "import torch\n",
    "from tensorRTCNN import generate_serialized_trt_engine\n",
    "\n",
    "pytorch_weights = torch.load(trained_pytorch_weights)\n",
    "print(pytorch_weights.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46c577c7",
   "metadata": {},
   "source": [
    "**TODO:** After you have finished implementing the TRT MNIST classifier in **generate_serialized_trt_engine()**, use **serializeMNIST()** to (i) load the trained PyTorch weights, (ii) generate the serialized engine, and (iii) save the serialized engine to a file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "97de4a3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading PyTorch weights.\n",
      "Saving serialized TRT engine.\n"
     ]
    }
   ],
   "source": [
    "# TODO: call serializeMNIST() to generate the serialized MNIST classifier TRT engine\n",
    "serializeMNIST(trained_pytorch_weights, trt_mnist_engine)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e697af3",
   "metadata": {},
   "source": [
    "As you've learned, the Jetson Nano has very limited memory resources. It's therefore a good idea to restart the kernel with `Kernel` --> `Restart Kernel...` to free as much memory as possible for the next steps. You might not need to, but if you run into OOM errors, try restarting here. Once you have generated the serial engine file you do not need to repeat the weight loading and inference optimization process. \n",
    "\n",
    "**TODO:** Load the serialized engine in the cell below with **load_serialized_engine()**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9bc6235b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading deserialized TRT engine.\n"
     ]
    }
   ],
   "source": [
    "# TODO: Use load_serialized_engine() to load the serialized engine into memory\n",
    "\n",
    "trt_serialized_engine = load_serialized_engine(trt_mnist_engine)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13bb99e6",
   "metadata": {},
   "source": [
    "**TODO:** Deserialize the TRT engine with [**runtime.deserialize_cuda_engine()**](https://docs.nvidia.com/deeplearning/tensorrt/api/python_api/infer/Core/Runtime.html#tensorrt.Runtime.deserialize_cuda_engine)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d19837ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: generate an ICudaEngine with runtime.deserialize_cuda_engine()\n",
    "\n",
    "trt_deserialized_engine = runtime.deserialize_cuda_engine(trt_serialized_engine)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beac8789",
   "metadata": {},
   "source": [
    "**TODO:** Calculate the accuracy of the TensorRT MNIST Classifier on `TEST_CASE` validation images. You can use **trt_prediction()** in **serializeMNIST.py** to generate random ground truth and predictions, or you can use **allocate_buffers()** and **do_inference()** to allocate input/output memory buffers and make predictions without choosing validation images randomly. Either approach is fine here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "83058b6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of trt model:98.61999999999999%\n"
     ]
    }
   ],
   "source": [
    "# TODO: calculate accuracy of TRT MNIST classifier on the validation set\n",
    "num = 5000\n",
    "count = 0\n",
    "for _ in range(num):\n",
    "    target, y_hat = trt_prediction(trt_deserialized_engine)\n",
    "    if target==y_hat:\n",
    "        count += 1\n",
    "acc = count/num\n",
    "print(\"Accuracy of trt model:{:2f}%\".format(acc*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d100a339",
   "metadata": {},
   "source": [
    "## Discussion\n",
    "\n",
    "### In what situation would converting a model to TensorRT be useful?\n",
    "\n",
    "**TODO:** Your answer here.\n",
    "\n",
    "\"TensorRT is an SDK for high-performance deep learning inference that delivers low latency and high throughput for inference applications.\" The TensoRT is very useful for deploying models in the situation that requires real-time. I'm really impressed that the yolov4-tiny model was accelerated for about 10 times in this lab. That means even though the deep model was accelerated by gpu hardware, there's still a lot of things can be optimized. From the website of TensorRT, even those tasks that doesn't require real-time like search engines are accelerated for more than 5 times which can greatly improve the user experience.\n",
    "\n",
    "### How does the PyTorch model validation accuracy compare to the TensorRT model validation accuracy?\n",
    "\n",
    "**TODO:** Your answer here.\n",
    "\n",
    "The validation accuracy of trt model is pretty much the same compared to the pytorch model.\n",
    "\n",
    "### Briefly explain how to following inference optimization techniques can increase the throughput of a model.\n",
    "\n",
    "#### Reduced Floating Point Precision\n",
    "\n",
    "**TODO:** Your answer here.\n",
    "\n",
    "The complexity of multiplying two floating point is $O(n^2)$ where $n$ is the number of bits of floating point numbers (or other complexities by various methods, but all complexities are monotonically increasing functions of length $n$). We know that deep learning models is redundant and doesn't need to be supper accurate. So reduce the precision of parameters is acceptable and can reduce the actual running time. \n",
    "\n",
    "reference: https://en.wikipedia.org/wiki/Computational_complexity_of_mathematical_operations\n",
    "\n",
    "#### Layer Fusion\n",
    "\n",
    "**TODO:** Your answer here.\n",
    "\n",
    "An unoptimized computational graph will do computation nodes by nodes, but there are adjacent layers can be fused and therefore save the time of transfering data such as convolution layer and ReLu activation function. In the building process of TensorRT, it will fuse convolution layer and ReLu to form a new layer and replace the original two layers which has the same result but save the transfering time. \n",
    "\n",
    "So in building process of TensorRT, it detects supported types of layers that can be fused and construct a new layer replacing them. The types of supported fusions can be found in this reference.\n",
    "\n",
    "reference: https://docs.nvidia.com/deeplearning/tensorrt/developer-guide/index.html#layer-fusion\n",
    "\n",
    "#### Dynamic Tensor Memory\n",
    "\n",
    "**TODO:** Your answer here.\n",
    "\n",
    "TensorRT reduces memory footprint and improves memory reuse by designating memory for each tensor only for the duration of its usage, avoiding memory allocation overhead for fast and efficient execution.\n",
    "\n",
    "reference: https://developer.nvidia.com/blog/tensorrt-3-faster-tensorflow-inference/\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
