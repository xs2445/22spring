{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8e64aaf0",
   "metadata": {},
   "source": [
    "## Lab - TensorRT + Profiling - Profiling\n",
    "## E6692 Spring 2022"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf9e15d2",
   "metadata": {},
   "source": [
    "In this part you will write two Python scripts: one to do inference using the PyTorch YOLOv4-Tiny model `pytorch_inference.py` and one to do inference using the TensorRT YOLOv4-Tiny model `trt_inference.py`. Use the following guidelines when writing these scripts:\n",
    "\n",
    "* Model weights and configuration file paths should be passed as command line arguments. Use `sys.argv` to manage the command line arguments.\n",
    "* Use the OpenCV function`cv2.VideoCapture()` to read frames from the original video and `cv2.VideoWriter()` to write frames to the output file. \n",
    "* Measure the inference speed of the model and the end-to-end speed of the script including **reading/frame preprocess/inference/postprocess/frame write** with the `time` module. You're welcome to do more in depth timing, but only end-to-end and inference timing are required. Record the measurements by populating the table below.\n",
    "* Generate a detected version of the 1st floor intersection video **test-lowres.mp4**. The output video names should be **test-lowres-pytorch-detected.mp4** and **test-lowres-tensorrt-detected.mp4**, respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49eca0c1",
   "metadata": {},
   "source": [
    "| Model Type | Model Input Size | Inference Speed (FPS) | End-to-end speed (FPS) |\n",
    "| --- | --- | --- | --- |\n",
    "| PyTorch | TODO | TODO | TODO |\n",
    "| TensorRT | TODO | TODO | TODO |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95dfba53",
   "metadata": {},
   "source": [
    "After you've written the video detection scripts and visually inspected the output for correctness, the next step is to perform CUDA profiling to give some insights into how each program is performing. For the lab we will use the `nvprof` command line profiling tool. Go through the [user guide](https://docs.nvidia.com/cuda/profiler-users-guide/index.html) to familiarize yourself with `nvprof`.\n",
    "\n",
    "Profiling tools give insights into specific metrics pertaining to memory usage, computational bottlenecks, and power consumption. \n",
    "\n",
    "**TODO:** Enter the command `nvprof --query-metrics` to list metrics available for profiling. Choose three that you think could be useful for our use case and describe what they indicate about the program."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e8019a5",
   "metadata": {},
   "source": [
    "A useful feature for identifying where a program could be further optimized is the [dependency analysis](https://docs.nvidia.com/cuda/profiler-users-guide/index.html#dependency-analysis) tool. Briefly explain what the dependency analysis tool does.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "061b0315",
   "metadata": {},
   "source": [
    "**TODO:** Your answer here. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1045751c",
   "metadata": {},
   "source": [
    "Next, you will profile your scripts `pytorch_inference.py` and `trt_inference.py`. To profile from the command line enter `nvprof <profiling_options> python3 <script_options>`. You should specify `--unified-memory-profiling off` to disable unified memory profiling (not supported for Jetson Nano) and `--dependency-analysis` to generate the dependency analysis report. Output the profiling results to text files `profiling_torch_log.txt` and `profiling_trt_log.txt` by including `--log-file <txt_file_path>` in the profiling options. \n",
    "\n",
    "**TODO:** Profile `pytorch_inference.py` and `trt_inference.py` to the specifications outlined above."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58917b5a",
   "metadata": {},
   "source": [
    "## Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "312b2047",
   "metadata": {},
   "source": [
    "### Provide commentary on the results of the inference speed and the end-to-end speed measurements for the two detection scripts.\n",
    "\n",
    "**TODO:** Your answer here.\n",
    "\n",
    "### Identify some differences between the TensorRT and the PyTorch script profile output.\n",
    "\n",
    "**TODO:** Your answer here.\n",
    "\n",
    "### What, if anything, does the dependency analysis indicate can be optimized in each of the detection scripts?\n",
    "\n",
    "**TODO:** Your answer here."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
