{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e69067fa",
   "metadata": {},
   "source": [
    "## Lab - TensorRT + Profiling - Profiling\n",
    "## E6692 Spring 2022"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5bdd91a",
   "metadata": {},
   "source": [
    "In this part you will write two Python scripts: one to do inference using the PyTorch YOLOv4-Tiny model `pytorch_inference.py` and one to do inference using the TensorRT YOLOv4-Tiny model `trt_inference.py`. Use the following guidelines when writing these scripts:\n",
    "\n",
    "* Model weights and configuration file paths should be passed as command line arguments. Use `sys.argv` to manage the command line arguments.\n",
    "* Use the OpenCV function`cv2.VideoCapture()` to read frames from the original video and `cv2.VideoWriter()` to write frames to the output file. \n",
    "* Measure the inference speed of the model and the end-to-end speed of the script including **reading/frame preprocess/inference/postprocess/frame write** with the `time` module. You're welcome to do more in depth timing, but only end-to-end and inference timing are required. Record the measurements by populating the table below.\n",
    "* Generate a detected version of the 1st floor intersection video **test-lowres.mp4**. The output video names should be **test-lowres-pytorch-detected.mp4** and **test-lowres-tensorrt-detected.mp4**, respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27a62b34",
   "metadata": {},
   "source": [
    "| Model Type | Model Input Size | Inference Speed (FPS) | End-to-end speed (FPS) |\n",
    "| --- | --- | --- | --- |\n",
    "| PyTorch | (960,540,3) | 1.09 | 0.92 |\n",
    "| TensorRT | (960,540,3) | 10.45 | 4.99 |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23db9b04",
   "metadata": {},
   "source": [
    "After you've written the video detection scripts and visually inspected the output for correctness, the next step is to perform CUDA profiling to give some insights into how each program is performing. For the lab we will use the `nvprof` command line profiling tool. Go through the [user guide](https://docs.nvidia.com/cuda/profiler-users-guide/index.html) to familiarize yourself with `nvprof`.\n",
    "\n",
    "Profiling tools give insights into specific metrics pertaining to memory usage, computational bottlenecks, and power consumption. \n",
    "\n",
    "**TODO:** Enter the command `nvprof --query-metrics` to list metrics available for profiling. Choose three that you think could be useful for our use case and describe what they indicate about the program."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7294510d",
   "metadata": {},
   "source": [
    "A useful feature for identifying where a program could be further optimized is the [dependency analysis](https://docs.nvidia.com/cuda/profiler-users-guide/index.html#dependency-analysis) tool. Briefly explain what the dependency analysis tool does.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84d15426",
   "metadata": {},
   "source": [
    "**TODO:** Your answer here. \n",
    "\n",
    "The dependency analysis tells us the timeline of each API calls and CUDA kernels on cpu and gpu. Two important variables are <strong>critical path</strong> and <strong>waiting time</strong>.\n",
    "\n",
    "<strong>Critical path</strong> denotes the longest path through an event graph that does not contain wait states. So optimizing activities on this path can directly improve the execution time.\n",
    "\n",
    "<strong>Waiting time</strong> denotes the duration for which an activity is blocked waiting on an event in another thread or stream. Waiting time is an inidicator for load-imbalances between execution streams.\n",
    "\n",
    "This provide us a view that wether the cpu or gpu are waiting for results when the other is running some process which cost an extra waiting time. We need to focus on those functions that takes high portion of critical path or waiting time. Try to overlap the waiting time and find ways to improve the time of critical path."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0508645",
   "metadata": {},
   "source": [
    "Next, you will profile your scripts `pytorch_inference.py` and `trt_inference.py`. To profile from the command line enter `nvprof <profiling_options> python3 <script_options>`. You should specify `--unified-memory-profiling off` to disable unified memory profiling (not supported for Jetson Nano) and `--dependency-analysis` to generate the dependency analysis report. Output the profiling results to text files `profiling_torch_log.txt` and `profiling_trt_log.txt` by including `--log-file <txt_file_path>` in the profiling options. \n",
    "\n",
    "**TODO:** Profile `pytorch_inference.py` and `trt_inference.py` to the specifications outlined above."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27b85a13",
   "metadata": {},
   "source": [
    "## Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a444da86",
   "metadata": {},
   "source": [
    "### Provide commentary on the results of the inference speed and the end-to-end speed measurements for the two detection scripts.\n",
    "\n",
    "**TODO:** Your answer here.\n",
    "\n",
    "TensorRT has a significant acceleration on inferencing speed, almost 10x faster! Also about 5x faster on end-to-end inferencing. As the time for inference is a samll portion of end-to-end process for trt model, the resizing, reading and writing of frames has a significant effect on the end-to-end speed.\n",
    "\n",
    "### Identify some differences between the TensorRT and the PyTorch script profile output.\n",
    "\n",
    "**TODO:** Your answer here.\n",
    "\n",
    "Except for Profiling result and Dependency Analysis, the profile of trt has an additional section \"NVTX result\" which gives an detailed profile of NVIDIAÂ® Tools Extension SDK. In this section, the information of each trt layer is shown in TensorRT domain so that we can directly look into this domain to see the performance of that package.\n",
    "\n",
    "There's no single ReLu layer in the trt model which indicates that trt has fused conv layer and relu activation funciton. Also the fused layer cost less time than the conv layer in pytorch.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### What, if anything, does the dependency analysis indicate can be optimized in each of the detection scripts?\n",
    "\n",
    "**TODO:** Your answer here.\n",
    "\n",
    "Picked functions:\n",
    "\n",
    "<center><strong>TensorRT Model:</strong></center>\n",
    "\n",
    "|Critical path(%)|  Critical path|  Waiting time|  Name|\n",
    "| ---|---|---|---|\n",
    "|37.41%|    197.397023s|           0ns|  cudaMalloc|\n",
    "|33.19%|    175.129550s|           0ns|  cuCtxDetach|\n",
    "|14.01%|     73.946650s|           0ns|  \\<Other\\>|\n",
    "|7.82%|     41.253897s|           0ns|  cudaStreamCreateWithFlags_v5000|\n",
    "|4.06%|     21.420142s|           0ns|  cudaFree|\n",
    "|1.19%|      6.273914s|           0ns|  trt_maxwell_fp16x2_hcudnn_winograd_fp16x2_128x128_ldg1_ldg4_relu_tile148m_nt_v1|\n",
    "|1.09%|      5.726921s|           0ns|  cudaMemGetInfo|\n",
    "|0.01%|    48.155303ms|   16.082453ms|  cudaStreamSynchronize|\n",
    "|0.00%|    29.532000us|     8.653162s|  cuStreamSynchronize|\n",
    "\n",
    "\n",
    "<center><strong>Pytorch Model:</strong></center>\n",
    "\n",
    "|Critical path(%)|  Critical path|  Waiting time|  Name|\n",
    "| ---|---|---|---|\n",
    "|41.29%|    223.927519s|           0ns|  cudaMalloc|\n",
    "|17.94%|     97.284251s|           0ns|  \\<Other\\>|\n",
    "|14.51%|     78.691248s|           0ns|  cudaLaunchKernel_v7000|\n",
    "|11.59%|     62.867639s|           0ns|  cudaStreamCreateWithFlags_v5000|\n",
    "|10.43%|     56.589771s|           0ns|  cuModuleUnload|\n",
    "|2.19%|     11.884091s|           0ns|  maxwell_scudnn_winograd_128x128_ldg1_ldg4_mobile_relu_tile148t_nt_v0|\n",
    "|0.33%|      1.776917s|           0ns|  maxwell_scudnn_128x64_relu_medium_nn_v1|\n",
    "|0.12%|   645.788588ms|   53.231000us|  cudaMemcpyAsync|\n",
    "|0.01%|    48.755587ms|    12.524525s|  cudaStreamSynchronize|\n",
    "\n",
    "For both model, the device memory allocation, context detachment and kernel launch cost largest portion of time, but those are not included in the inferencing time, it's a kind of overhead that every time launches a model. \n",
    "\n",
    "Synchronizations are the only functions that has a waiting time, but they takes reletively small portion of time in the critical path.\n",
    "\n",
    "We can see that the trt fused conv layer and relu and takes less time than pytorch model which is an great improvement. \n",
    "\n",
    "I'm kind of curious about the meaning of \\<other\\>, I guess those are not nvidia functions. Everything's managed by either trt or pytorch, without a further investigation of each functions and memory allocations, it's hard to further improve the running time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da979aca",
   "metadata": {},
   "source": [
    "<strong>Commands:</strong>\n",
    "\n",
    "trt:\n",
    "\n",
    "python3 trt_inference.py -t \"./engines/yolov4-tiny-person-vehicle.trt\"\n",
    "\n",
    "nvprof --unified-memory-profiling off --dependency-analysis --log-file \"profiling_trt_log.txt\" python3 trt_inference.py -t \"./engines/yolov4-tiny-person-vehicle.trt\"\n",
    "\n",
    "nvprof --metrics global_load_requests,flop_sp_efficiency,flop_dp_efficiency --unified-memory-profiling off --dependency-analysis --log-file \"profiling_trt_log.txt\" python3 trt_inference.py -t \"./engines/yolov4-tiny-person-vehicle.trt\"\n",
    "\n",
    "\n",
    "pytorch:\n",
    "\n",
    "python3 pytorch_inference.py -w \"./weights/yolov4-tiny-person-vehicle_best.weights\" -c \"./cfg/yolov4-tiny-person-vehicle.cfg\"\n",
    "\n",
    "nvprof --unified-memory-profiling off --dependency-analysis --log-file \"profiling_torch_log.txt\" python3 pytorch_inference.py -w \"./weights/yolov4-tiny-person-vehicle_best.weights\" -c \"./cfg/yolov4-tiny-person-vehicle.cfg\"\n",
    "\n",
    "nvprof --metrics global_load_requests,flop_sp_efficiency,flop_dp_efficiency --unified-memory-profiling off --dependency-analysis --log-file \"profiling_torch_log.txt\" python3 pytorch_inference.py -w \"./weights/yolov4-tiny-person-vehicle_best.weights\" -c \"./cfg/yolov4-tiny-person-vehicle.cfg\"\n",
    "\n",
    "\n",
    "<strong>Metrics:</strong>\n",
    "\n",
    "shared_load_transactions_per_request:  Average number of shared memory load transactions performed for each shared memory load\n",
    "\n",
    "warp_execution_efficiency:  Ratio of the average active threads per warp to the maximum number of threads per warp supported on a multiprocessor\n",
    "\n",
    "double_precision_fu_utilization:  The utilization level of the multiprocessor function units that execute double-precision floating-point instructions on a scale of 0 to 10\n",
    "\n",
    "single_precision_fu_utilization:  The utilization level of the multiprocessor function units that execute single-precision floating-point instructions and integer instructions on a scale of 0 to 10\n",
    "\n",
    "shared_efficiency:  Ratio of requested shared memory throughput to required shared memory throughput expressed as percentage\n",
    "\n",
    "global_load_requests:  Total number of global load requests from Multiprocessor\n",
    "\n",
    "local_load_requests:  Total number of local load requests from Multiprocessor\n",
    "\n",
    "local_memory_overhead:  Ratio of local memory traffic to total memory traffic between the L1 and L2 caches expressed as percentage\n",
    "\n",
    "flop_sp_efficiency:  Ratio of achieved to peak single-precision floating-point operations\n",
    "\n",
    "flop_dp_efficiency:  Ratio of achieved to peak double-precision floating-point operations"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
